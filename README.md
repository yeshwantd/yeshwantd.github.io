# Introduction

Soft Actor-Critic (SAC) was published in 2018, and it quickly became a
go-to baseline for continuous-control reinforcement learning. Its main
contribution is a clean, practical way to do off-policy (learn from data
generated by a different policy than the current policy) actor-critic
(learn both: how good an action is, and a policy that picks good
actions) reinforcement learning (RL) while also optimizing a
maximum-entropy objective - meaning the agent is rewarded not just for
high return, but also for keeping its policy stochastic enough to
explore. In this tutorial, we'll do a detailed walkthrough of the
algorithm with an emphasis on intuitive explanations, and we'll connect
each idea directly to what we implement in code. We'll start with
LunarLander from Gym as a friendly environment to get everything working
end-to-end, and once the core pieces make sense, we'll reuse the same
SAC setup and adapt it to a harder MuJoCo Humanoid environment.

# RL in a Nutshell

RL is a setup where an agent learns by interacting with an environment.
At each step, the agent chooses an action using a policy (often a neural
network, that maps what an agent observes to what it does). That action
changes the environment, leading to a new state that the agent can
observe - sometimes fully, and sometimes only partially if the agent has
limited information. Along the way, the agent receives a reward signal
that tells it how well things are going; this reward can come directly
from the environment (an external score), or it can be something the
agent defines for itself as an intrinsic motivation signal. Over time,
the learning algorithm updates the policy to reinforce actions that tend
to produce higher rewards, so the agent becomes more likely to repeat
behaviors that work and less likely to repeat ones that don't.

![RL Loop](SAC1.jpg)

# Policies, Rewards, and the RL Objective

A policy $\pi$ is the agent's decision rule. It tells the agent what
action $a$ to take when it sees a state (or observation) $s$. In general
we'll use a stochastic policy, which returns a distribution over actions
rather than a single action:

$$a \sim \pi(\cdot \mid s)$$

or equivalently

$$\pi(a\mid s) = \Pr(A=a \mid S=s).$$

You can also think of the policy as a function with parameters $\theta$
(usually neural network weights):

$$a \sim \pi_\theta(\cdot \mid s)$$

Next, the environment gives the agent a reward, which we'll treat as a
function of the current state and action:

$$r_t = r(s_t, a_t).$$

Environments sometimes make reward depend on the next state too,
$r(s_t,a_t,s_{t+1})$, but $r(s_t,a_t)$ is enough for our tutorial and
matches how we usually model it.

The goal of RL is to find a policy that gets high reward over time. To
make "over time" precise, we usually discount future rewards: rewards
that happen soon count more, and rewards far in the future count less.
This reflects the idea that the future is harder to predict and that we
often prefer getting value sooner rather than later.

$$G(\tau) = \sum_{t=0}^{\infty} \gamma^t \, r(s_t, a_t),
\quad \text{with } \gamma \in [0,1).$$

Here $\tau = (s_0,a_0,s_1,a_1,\ldots)$ is a trajectory generated by
rolling out the policy in the environment. Because the environment can
be stochastic, and the policy can be stochastic, $G(\tau)$ is a random
variable. So the RL objective is to maximize its expected value:

$$J(\pi) = \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{\infty} \gamma^t \, r(s_t, a_t)\right].$$

This expectation hides an important idea: the policy doesn't just choose
actions - it also shapes which states you visit. A useful way to capture
that is with the (discounted) state-action visitation distribution,
often called an occupancy measure:

$$\rho_\pi(s,a) \;=\; \sum_{t=0}^{\infty} \gamma^t \, \Pr(s_t = s,\, a_t = a \mid \pi).$$

Intuitively, $\rho_\pi(s,a)$ counts how often the policy visits each
$(s,a)$ pair, with earlier steps weighted more than later steps. Using
$\rho_\pi$, we can rewrite the same objective in a compact form:

$$J(\pi) = \mathbb{E}_{(s,a)\sim \rho_\pi}\big[\, r(s,a) \,\big].$$

That last line is the key setup for actor-critic methods: learn which
actions are good in which states, and then update the policy so it
visits good $(s,a)$ pairs more often. In the next sections, we'll build
the value functions that make those updates practical.

# Value Functions

To make those updates practical, we need a way to score how good an
action was in a state. The most direct idea is to run the policy,
collect a full trajectory, and then use the Monte Carlo return - the
actual discounted sum of rewards observed after that decision - as the
score:

$$G_t=\sum_{k=0}^{\infty}\gamma^k r_{t+k}.$$

In theory, $G_t$ tells us how good it is to take action $a_t$ in state
$s_t$. In practice, it's noisy: two trajectories that start in the same
state can end very differently with very different rewards, so our score
has high variance.

Actor--critic methods reduce that variance by introducing a learned
"helper" model: a value function (often a Q-function). The Q-function
answers a simple question: if I take action $a$ in state $s$, how much
reward do I expect to collect over time (under my current policy)? In
other words, $Q^\pi(s,a)$ is the expected sum of discounted rewards
under policy $\pi$ given we are in state $s$ and take action $a$.

$$Q^\pi(s,a)=\mathbb{E}_\pi\!\left[\sum_{k=0}^{\infty}\gamma^k r_{t+k}\ \middle|\ s_t=s, a_t=a\right]$$

Once we have $Q^\pi$, we don't have to wait until the end of an episode
to get a score. We can "bootstrap" from the next step using the Bellman
recursion:

$$Q^\pi(s_t,a_t)
=\mathbb{E}_{s_{t+1},\,r_t \sim p(\cdot \mid s_t,a_t)}
\Big[r_t+\gamma\ \mathbb{E}_{a_{t+1}\sim \pi(\cdot|s_{t+1})}\big[Q^\pi(s_{t+1},a_{t+1})\big]\Big]$$

The key trick is that the Bellman equation turns a long, noisy \"sum of
rewards until the end of the episode" into a one-step learning target.
Instead of waiting to see everything that happens in the future, we use
the reward we just observed, $r_t$, plus our current estimate of what
will happen next. This usually reduces variance, because the immediate
reward is fixed once we take the step, and the remaining "future part"
is summarized by a value estimate rather than a full rollout. In other
words, we trade some bias (we rely on an estimate of future rewards) for
a big gain in stability (we avoid wildly different trajectory outcomes).

The network that learns this value estimate is called the critic because
it plays the role of an evaluator: given a state and action, it
"critiques" how good that choice is by predicting the expected long-term
return. The actor is the policy because it's the component that actually
acts in the environment - sampling actions and generating behavior.
Training alternates between these roles: the critic learns to score
decisions more accurately, and the actor learns to choose decisions that
score well according to the critic.

# The Actor-Critic Loop

In this section, we're trying to do two things at the same time: learn
how good different actions are, and use that knowledge to improve the
way we act. The loop looks like this:

1.  Collect experience. Run the current policy in the environment and
    record what happens.

2.  Learn a value model ("critic"). Use those records to train a
    Q-function that predicts long-term reward.

3.  Improve the policy ("actor"). Use the Q-function's feedback to
    update the policy so it chooses better actions more often.

**Step 1 - Collect experience (and store it)**

We roll out our current policy $\pi_\theta$ and collect experience
tuples: $(s_t, a_t, r_t, s_{t+1}, d_{t+1})$ where $d_{t+1}$ is a "done"
flag (episode ended). We push these tuples into a replay buffer. The
buffer matters because it lets us reuse past experience many times,
which is much more sample-efficient than throwing data away after one
update.

**Step 2 - Learn the critic $Q_\phi(s,a)$**

We want the critic $Q_\phi(s,a)$ to answer a concrete question: if we
take action $a$ in state $s$, how much discounted reward should we
expect to collect from that point onward? To train a neural network to
predict this quantity, we need a target - a value we want the network's
prediction to match.

A simple and practical target comes from the Bellman recursion: the
value of $(s_t,a_t)$ should equal the reward we get right now, plus the
discounted value of what happens next. That gives the target

$$y_t = r_t + \gamma (1-d_{t+1}) \ \mathbb{E}_{a' \sim \pi_\theta(\cdot \mid s_{t+1})}\big[Q_\phi(s_{t+1}, a')\big]$$

Here $d_{t+1}$ is 1 if the episode ended at $t+1$, in which case there
is no "next" value to add.

Once we have this target $y_t$, we can learn $Q_\phi$ by making its
predictions match $y_t$ on data from the replay buffer $\mathcal{D}$. A
standard choice is mean squared error:

$$\mathcal{L}_Q(\phi) = \mathbb{E}_{(s,a,r,s',d)\sim \mathcal{D}}\Big[\big(Q_\phi(s,a) - y\big)^2\Big]$$

There is one practical issue with the target above: if we compute $y_t$
using the same network $Q_\phi$ that we are updating, then every
gradient step changes not only the prediction $Q_\phi(s,a)$, but also
the target we are trying to match. This "moving target" effect can make
learning unstable.

To stabilize training, we introduce a second set of parameters
$\bar{\phi}$ and compute the target using a slowly changing copy of the
critic:

$$y_t = r_t + \gamma (1-d_{t+1}) \ \mathbb{E}_{a' \sim \pi_\phi(\cdot \mid s_{t+1})}\big[Q_{\bar{\phi}}(s_{t+1}, a')\big]$$

We still update $Q_\phi$ to fit this target, but we update $\bar{\phi}$
slowly so the target changes smoothly over time, for example:

$$\bar{\phi} \leftarrow \tau \phi + (1-\tau)\bar{\phi},
\quad \text{with } \tau \ll 1$$

This way, the critic learns using a target that doesn't jump around
every step, which usually makes training far more stable.

**Step 3 - Update the actor $\pi_\theta(a|s)$ using the critic**

Once the critic can reasonably estimate how good $(s,a)$ is, we can
improve the policy. The core idea behind policy gradients is simple:

-   If an action leads to better-than-expected outcomes, make the policy
    more likely to take it again in that state.

-   If an action leads to worse-than-expected outcomes, make the policy
    less likely to take it.

A common way to define "better than expected" is the advantage:

$$A(s_t,a_t) = Q_\phi(s_t,a_t) - V(s_t)$$

Here $V(s_t)$ is what you "expect" to get from state $s_t$ on average -
formally, it's the expected action-value under the policy:

$$V(s_t) = \mathbb{E}_{a \sim \pi_\theta(\cdot \mid s_t)}\big[Q_\phi(s_t,a)\big]$$

So $A(s_t,a_t)$ measures how much better (or worse) the specific action
$a_t$ is compared to the policy's typical action at that state. That's
exactly why it's called an advantage: it tells you whether $a_t$ has an
advantage over the "average" action choice at $s_t$.

Using the advantage also tends to reduce variance compared to using raw
$Q_\phi(s_t,a_t)$ directly. Intuitively, Q-values can be large and can
swing a lot because they include everything about how good the state is
overall. Subtracting $V(s_t)$ removes that shared "state is generally
good / generally bad" part, leaving a more focused signal about the
relative quality of the chosen action. That makes the gradient updates
less noisy and usually helps learning move in a steadier direction.

The policy gradient update is:

$$\nabla_\theta J(\theta)
= \mathbb{E}_{s_t \sim \rho_{\pi_\theta},\ a_t \sim \pi_\theta(\cdot \mid s_t)}
\Big[\nabla_\theta \log \pi_\theta(a_t \mid s_t)\, A(s_t,a_t)\Big]$$

Intuition for that equation:
$\nabla_\theta \log \pi_\theta(a_t \mid s_t)$ nudges the policy to
increase the probability of the sampled action $a_t$. Multiplying by
$A(s_t,a_t)$ decides the direction and size of the nudge: positive
advantage pushes probability up, negative advantage pushes it down.

# Entropy Regularization: Key Idea Behind SAC

So far, our objective has been "get as much reward as possible." SAC
keeps that goal, but adds one extra preference: don't become too certain
too early. It does this by regularizing the RL objective with the
entropy of the policy. In plain terms, entropy measures how random (or
spread out) the policy is. A high-entropy policy keeps multiple actions
"on the table," while a low-entropy policy collapses to always doing the
same thing.

For a stochastic policy $\pi(a\mid s)$, the (Shannon) entropy at a state
$s$ is typically written as

$$\mathcal{H}(\pi(\cdot\mid s)) \;=\; -\mathbb{E}_{a\sim \pi(\cdot\mid s)}\big[\log \pi(a\mid s)\big]$$

If the policy is very peaked (almost deterministic), $\log \pi(a\mid s)$
is close to 0 for one action and very negative for others, and the
entropy is very low. If the policy spreads probability across many
reasonable actions, entropy is higher.

Why add this to the objective? The SAC paper motivates maximum-entropy
RL as a way to (1) encourage broader exploration, (2) represent multiple
near-optimal behaviors instead of committing to just one, and (3)
improve learning speed and robustness in practice. More generally,
entropy maximization pushes the agent to find policies that get good
reward and keep flexibility - useful when the critic is imperfect or the
environment is noisy.

Putting reward and entropy together gives the SAC objective (Equation 1
in the paper):

$$J(\pi) \;=\; \sum_{t=0}^{T}\ \mathbb{E}_{(s_t,a_t)\sim \rho_\pi}\Big[\, r(s_t,a_t)\;+\;\alpha\,\mathcal{H}(\pi(\cdot\mid s_t)) \Big]$$

This is the standard expected-reward objective, but with an added
entropy bonus. The coefficient $\alpha$ is the temperature: it sets the
tradeoff between "get reward" and "stay stochastic." Larger $\alpha$
encourages more randomness; as $\alpha \to 0$, we recover the usual RL
objective without entropy.

# SAC Algorithm Overview

The SAC paper includes an Algorithm, but it's intentionally high-level.
It's useful as a map of the moving parts, yet it leaves out many
practical details we need for a clean implementation (like replay-buffer
sampling, exact targets, and the update schedule). So for the rest of
this tutorial, we'll treat the paper's algorithm as a reference point,
but we'll follow the more explicit [Soft Actor-Critic pseudocode from
OpenAI Spinning
Up](https://spinningup.openai.com/en/latest/algorithms/sac.html#pseudocode)
and code it up line by line.

#### Algorithm from the SAC paper
![Algorithm from the SAC paper](SAC_algo_paper.png)

#### Algorithm from Spinning Up
![image](SAC_from_Spinning_Up.png)

# Coding SAC

## Lunar Lander Environment

We'll use gym's LunarLanderContinuous environment, where the agent
controls a small lander using continuous thrusters.

-   State (observation) is 8-dimensional. It includes the lander's
    position $(x, y)$, velocity $(v_x, v_y)$, angle, angular velocity,
    and two contact flags indicating whether each leg is touching the
    ground.

-   Action is 2-dimensional, with each value in \[-1, 1\]. The two
    numbers control the main engine throttle and the lateral (side)
    throttle.

Now we'll code it up line by line

## Line 1

**Input: initial policy parameters $\theta$, Q-function parameters
$\phi_1, \phi_2$, empty replay buffer $\mathcal{D}$**

Let's start with defining the policy class.

``` python
class PolicyNetwork(nn.Module):
    def __init__(self):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(8, 256)
        self.fc2 = nn.Linear(256, 256)
        self.mean = nn.Linear(256, 2) 
        self.log_std = nn.Linear(256, 2)

    def forward(self, x):
        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        mean = self.mean(x) 
        log_std = self.log_std(x)
        log_std = torch.clamp(log_std, min=-20, max=2)
        return mean, log_std
```

In LunarLanderContinuous, the policy takes an 8-dimensional state as
input and outputs a 2-dimensional action (main throttle and lateral
throttle). That's why the first linear layer maps from 8 features, and
why the network outputs two values for the action distribution
parameters (mean and log standard deviation) in 2 dimensions. We model
the policy as a Gaussian (normal) distribution because our action space
is continuous, and a normal distribution is a simple, standard way to
represent it. The network predicts the mean and the log standard
deviation instead of the standard deviation directly, because the
standard deviation must be positive. Predicting $\log \sigma$ lets us
turn it into $\sigma = \exp(\log \sigma)$ while keeping optimization
stable, and clamping $\log \sigma$ prevents the policy from becoming
either almost deterministic or wildly random too early.

Next, we define the Q-function. A Q-function $Q(s,a)$ takes a state and
an action, and outputs a single number: its estimate of the discounted
sum of future rewards you can expect after taking that action in that
state. We concatenate the state and actions into a 10-dimensional input
vector \[s, a\]. The network outputs a scalar value.

``` python
class QNetwork(nn.Module):
    def __init__(self):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(10, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 1)
        
    def forward(self, x):
        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

We initialize the gym environment, instantiate the policy and two
Q-functions (we'll explain why two Q-functions later), and the empty
replay buffer

``` python
env = gym.make("LunarLanderContinuous-v3", max_episode_steps=500)
policy = PolicyNetwork()
q1 = QNetwork()
q2 = QNetwork()
replay_buffer = deque(maxlen=100000)
```

`max_episode_steps` puts a hard cap on how long an episode can run. This
prevents episodes from going on forever when the agent is doing poorly,
keeps training batches a consistent size, and makes learning more stable
by ensuring we regularly reset and collect fresh experience.

## Line 2

**Set target parameters equal to main parameters
$\phi_{targ,1} \leftarrow \phi_1, \phi_{targ,2} \leftarrow \phi_2$**

``` python
q1_target = copy.deepcopy(q1)
q2_target = copy.deepcopy(q2)
```

As mentioned in Step 2 of the Actor-Critic Loop section (section 5), we
keep separate target critic parameters to make the bootstrap target in
the Bellman backup more stable. If the target uses the same parameters
that we are updating in the current gradient step, the "label" we are
trying to fit is moving at the same time as the predictor, which can
cause instability and divergence. By computing targets with a delayed
copy (and later updating it slowly), we turn the critic update into a
much better-behaved regression problem.

## Lines 4-8

**4: Observe state $s$ and select action
$a \sim \pi_\theta(\cdot \mid s)$\
5: Execute $a$ in the environment\
6: Observe next state $s'$, reward $r$, and done signal $d$ to indicate
whether $s'$ is terminal\
7: Store $(s,a,r,s',d)$ in replay buffer $\mathcal{D}$\
8: If $s'$ is terminal, reset environment state**

``` python
for i in range(num_trajectories):
    obs, info = env.reset()
    done = False
    while not done:
        with torch.no_grad():
            action, _ = policy.sample(torch.tensor(obs, dtype=torch.float32).unsqueeze(0))
        action = action.squeeze(0).numpy()
        next_obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        replay_buffer.append((obs, action, reward, next_obs, done))
        obs = next_obs
```

The SAC algorithm shows a single "collect one step, then update" loop.
In our implementation, we collect experience in chunks by running
`num_trajectories` episodes. This gives us a reasonable amount of data
in the replay buffer before we start taking gradient steps, and it keeps
the code structure simple: collect → train → repeat.

Notice we call `policy.sample(...)` instead of `policy.forward(...)`.
This is a new helper that actually draws an action from the policy's
Gaussian distribution (instead of just returning the mean and standard
deviation). We'll break down sample() next, since it also computes the
action log-probability that SAC needs.

Two small implementation details to keep in mind: we use
`torch.no_grad()` during data collection because we are not training the
networks in this loop, and we treat both terminated and truncated as
done so episodes end either when the task finishes or when the time
limit is reached.

Let's add the `sample(...)` method to our `PolicyNetwork` class.

``` python
class PolicyNetwork(nn.Module):
    def __init__(self):
        ...

    def forward(self, x):
        ...

    def sample(self, x):
        mean, log_std = self.forward(x)
        std = log_std.exp()
        normal = Normal(mean, std)
        u = normal.rsample() 
        action = torch.tanh(u)
        log_prob = normal.log_prob(u)
        log_prob -= torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        return action, log_prob
```

sample(x) takes x, which is the state (or a batch of states). We run it
through the policy network to get the parameters of a Gaussian
distribution over actions:

-   `mean, log_std = self.forward(x)` produces two 2D vectors (because
    actions are 2D).

-   We predict log standard deviation for stability, then convert it
    back with `std = log_std.exp()`. Exponentiating ensures
    $\sigma > 0$.

Next we create the Gaussian and draw a differentiable sample:

-   `normal = Normal(mean, std)` defines $\mathcal{N}(\mu, \sigma)$.

-   `u = normal.rsample()` samples an action using the
    reparameterization trick, which you can think of as
    $u = \mu + \sigma \odot \epsilon,\quad \epsilon \sim \mathcal{N}(0,I)$.

This keeps the sampling step differentiable, which matters because SAC
updates the policy with gradients.

Why do we apply tanh? The environment expects each action component to
lie in \[-1, 1\]. A Gaussian sample u can be any real number, so we
squash it using $a = \tanh(u)$. tanh smoothly maps $(-\infty,\infty)$ to
$(-1,1)$. This avoids hard clipping, which can create bad gradients and
weird behavior near the action limits.

After we apply tanh, the final action $a$ is not Gaussian anymore, even
though it came from a Gaussian $u$. But SAC needs $\log \pi(a\mid s)$
for the entropy term, so we must compute the log-probability of the
squashed action.

We start with the log-probability of the unsquashed sample under the
Gaussian $\log \mathcal{N}(u;\mu,\sigma)$ which is what
`log_prob = normal.log_prob(u)` gives. Now we correct it to account for
the squashing $a=\tanh(u)$. The key idea is that tanh stretches and
compresses space depending on where you are: near 0 it's steep, near
$\pm$ 1 it flattens out. The slope of tanh is:
$\frac{da}{du} = 1 - \tanh^2(u) = 1 - a^2$.

When you transform a random variable, probabilities must adjust by this
slope. In log space, that shows up as subtracting
$\log\left|\frac{da}{du}\right|$:
$\log \pi(a) = \log \mathcal{N}(u;\mu,\sigma) - \log(1-a^2)$.

That is exactly what our code does:

`log_prob -= torch.log(1 - action.pow(2) + 1e-6)`

-   `1 - action.pow(2)` is $1-a^2$.

-   `+ 1e-6` prevents numerical issues when action gets extremely close
    to $\pm$ 1.

Finally, because the action has two dimensions, `normal.log_prob(u)`
returns a log-prob per dimension. We sum them to get a single
log-probability per state: `log_prob = log_prob.sum(1, keepdim=True)`.
This is because for independent dimensions, probabilities multiply.
Taking logs turns that product into a sum: $\log \pi(u\mid s)
= \log \mathcal{N}(u_1;\mu_1,\sigma_1) + \log \mathcal{N}(u_2;\mu_2,\sigma_2)$.

So `sample(x)` returns:

-   `action`: a valid bounded action in $[-1,1]^2$

-   `log_prob`: the correct $\log \pi_\theta(a\mid s)$ for that squashed
    action, which SAC uses in the entropy-regularized objective

## Line 11

**Randomly sample a batch of transitions, $B = (s,a,r,s',d)$ from
$\mathcal{D}$**

``` python
batch = random.sample(replay_buffer, batch_size)
obs, action, reward, next_obs, done = zip(*batch)
obs = torch.tensor(np.array(obs), dtype=torch.float32)
action = torch.tensor(np.array(action), dtype=torch.float32)
reward = torch.tensor(np.array(reward), dtype=torch.float32).unsqueeze(1)
next_obs = torch.tensor(np.array(next_obs), dtype=torch.float32)
done = torch.tensor(np.array(done), dtype=torch.int8).unsqueeze(1)
```

We convert the NumPy arrays to PyTorch tensors for our networks and
gradient-based updates.

## Line 12

**Compute the targets for the Q-functions** $$y(r, s', d)
= r + \gamma(1-d)\left(
\min_{i=1,2} Q_{\phi_{\text{targ},i}}\!\left(s', \tilde a'\right)
- \alpha \log \pi_\theta\!\left(\tilde a' \mid s'\right)
\right),
\qquad
\tilde a' \sim \pi_\theta(\cdot \mid s')$$

``` python
with torch.no_grad():
    next_action, next_log_prob = policy.sample(next_obs)
    next_state_action = torch.cat([next_obs, next_action], dim=1)
    q1_next = q1_target(next_state_action)  
    q2_next = q2_target(next_state_action)  
    min_q_next = torch.min(q1_next, q2_next)  
    y = reward + gamma * (1 - done) * (min_q_next - entropy_weight * next_log_prob)
```

We use two target Q-networks because a single learned critic tends to
become overly optimistic: small function-approximation errors can
repeatedly push Q-values upward, and the policy then chases these
inflated estimates. With two independent critics, it's less likely that
both make the same optimistic mistake on the same (s,a). When we build
the target, we take
$$\min\!\left(Q_{\phi_{\text{targ},1}}(s',a'),\;Q_{\phi_{\text{targ},2}}(s',a')\right)$$
as a simple, practical bias-correction. The minimum is a conservative
estimate of the next-step value, which helps prevent the "Q-value
runaway" effect and makes training more stable in practice.

We set `gamma` $\gamma$ (discount) to 0.99, which is the value used in
the paper's reported hyperparameters. We set `entropy_weight` $\alpha$
to 0.05 as a simple fixed temperature: it controls how much SAC values
entropy (randomness in actions) versus reward in both the policy and
critic targets. In the SAC paper, the authors note you can absorb
$\alpha$ into reward scaling (scaling rewards by $\alpha^{-1}$ has the
same effect).

## Line 13

**Update Q-functions by one step of gradient descent using**
$$\nabla_{\phi_i}\,\frac{1}{|B|}\sum_{(s,a,r,s',d)\in B}
\Bigl(Q_{\phi_i}(s,a) - y(r,s',d)\Bigr)^2,
\qquad \text{for } i=1,2$$

This is a simple mean square error loss function.

``` python
state_action = torch.cat([obs, action], dim=1)
q1_loss = F.mse_loss(q1(state_action), y)
q2_loss = F.mse_loss(q2(state_action), y)

q1_optim.zero_grad()
q1_loss.backward()
q1_optim.step()

q2_optim.zero_grad()
q2_loss.backward()
q2_optim.step()
```

## Line 14

**Update policy by one step of gradient ascent using
$$\nabla_{\theta}\,\frac{1}{|B|}\sum_{s\in B}
\left(
\min_{i=1,2} Q_{\phi_i}\!\left(s,\tilde a_\theta(s)\right)
-\alpha \log \pi_\theta\!\left(\tilde a_\theta(s)\mid s\right)
\right)$$ where $\tilde a_\theta(s)$ is a sample from
$\pi_\theta\!\left(\cdot \mid s\right)$ which is differentiable wrt
$\theta$ via the reparameterization trick**

``` python
new_action, log_prob = policy.sample(obs)
q1_new = q1(torch.cat([obs, new_action], dim=1))
q2_new = q2(torch.cat([obs, new_action], dim=1))
min_q_new = torch.min(q1_new, q2_new)

policy_loss = -(min_q_new - entropy_weight * log_prob).mean()  

policy_optim.zero_grad()
policy_loss.backward()
policy_optim.step()
```

Just like in the Q-target, we evaluate the sampled action with two
critics and take the minimum:
$$\min\big(Q_{\phi_1}(s,a),\; Q_{\phi_2}(s,a)\big)$$

This keeps the policy update conservative. If one critic is accidentally
too optimistic about an action, the minimum reduces the chance that the
policy chases that error.

We minimize
`policy_loss  = -(min_q_new - entropy_weight * log_prob).mean()`, which
is the same as maximizing the expression inside the parentheses:

$$\max_\theta\ \mathbb{E}\left[\min(Q_{\phi_1},Q_{\phi_2})\;-\;\alpha \log \pi_\theta(a|s)\right]$$

Intuitively, the $\min Q$ term says: "choose actions that the critic
predicts will lead to high long-term reward." If a sampled action gets a
higher Q-value, increasing its probability improves the objective, so
gradient descent pushes the policy toward making that action more likely
in that state. The entropy term $-\alpha \log \pi_\theta(a|s)$
encourages the policy to stay stochastic. If the policy collapses too
early to a single action, it may stop exploring and get stuck. The
entropy bonus rewards spreading probability across multiple reasonable
actions, which improves exploration and tends to make learning more
robust.

## Line 15

**Update the target networks with
$$\phi_{\text{targ},i} \leftarrow \rho\,\phi_{\text{targ},i} + (1-\rho)\,\phi_i,
\qquad \text{for } i=1,2$$**

``` python
for target_param, param in zip(q1_target.parameters(), q1.parameters()):
    target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)

for target_param, param in zip(q2_target.parameters(), q2.parameters()):
    target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
```

In our code we use `1.0 - tau` for $\rho$. We update the target
parameters directly (with `.data.copy_)` because the target networks are
not trained by gradient descent. They exist purely to produce a slowly
changing reference for the critic targets, which avoids the "moving
target" instability. Since this is a deterministic bookkeeping step -
not part of the loss, we don't want gradients flowing through it. We
just blend the weights to make the targets track the learned critics
smoothly.

## Putting it All Together

The full implementation is available in Appendix A and on GitHub Gist:\
<https://gist.github.com/yeshwantd/564162e91c644205c8de274ff104418a>\
\
Here is a short demo of the trained policy on LunarLanderContinuous:\
<https://youtu.be/rlamGUQbviw>

# From LunarLander to Humanoid

To get the code to work on humanoid, we mainly need to adapt it to a
much larger state/action space and a few MuJoCo-specific details -
action bounds, observation scaling, and entropy tuning.

## Humanoid state and action spaces

Humanoid has a high-dimensional observation (348 floats) that bundles
many pieces of robot state (joint positions/velocities, body
orientation, contact-related signals, etc.). The action is a
17-dimensional vector of joint torques, with each component bounded
between `env.action_space.low` (-0.4) and `env.action_space.high` (0.4).

## Make the networks dimension-agnostic

In LunarLander we hard-coded sizes (8-dim state, 2-dim action). For
Humanoid, we will read sizes from the environment and pass them into the
networks:

``` python
env = gym.make("Humanoid-v5", max_episode_steps=1000)
obs_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]

policy = PolicyNetwork(obs_dim, action_dim)
q1 = QNetwork(obs_dim, action_dim)
q2 = QNetwork(obs_dim, action_dim)
```

The policy outputs a mean and log-std of length `action_dim`.

``` python
class PolicyNetwork(nn.Module):
    def __init__(self, obs_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(obs_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.mean = nn.Linear(256, action_dim)
        self.log_std = nn.Linear(256, action_dim)
```

The Q-function input becomes `obs_dim + action_dim` (concatenate
$[s,a]$), and the output remains a single scalar.

``` python
class QNetwork(nn.Module):
    def __init__(self, obs_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(obs_dim + action_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 1)
```

## Rescale actions to match Humanoid torque bounds

Our policy produces actions in $[-1,1]$ because of the `tanh` squashing.
Humanoid typically expects torques in a smaller range
(`env.action_space.low/high`). A robust approach is to map $[-1,1]$ into
the environment bounds before calling `env.step(...)`:

``` python
env_action = env.action_space.low + (action + 1.0) * 0.5 * (env.action_space.high - env.action_space.low)
next_obs, reward, terminated, truncated, info = env.step(env_action)
```

## Use automatic entropy tuning instead of a fixed $\alpha$

Humanoid is more sensitive to the entropy coefficient. A common
improvement is to learn $\alpha$ automatically by targeting a desired
entropy level. A simple default is setting target entropy to minus the
number of action dimensions.

``` python
target_entropy = -torch.prod(torch.Tensor(env.action_space.shape).to(torch.device("cpu"))).item()
log_alpha = torch.zeros(1, requires_grad=True)
alpha_optim = optim.Adam([log_alpha], lr=alpha_lr)

# ... during training ...
alpha_loss = -(log_alpha * (log_prob + target_entropy).detach()).mean()
alpha_optim.zero_grad()
alpha_loss.backward()
alpha_optim.step()
alpha = log_alpha.exp()
```

## Normalize observations

Humanoid observations contain many components with different scales.
Using a running mean and variance normalizer often improves stability.

``` python
class RunningMeanStd:
    def __init__(self, shape):
        self.mean = np.zeros(shape)
        self.var = np.ones(shape)
        self.count = 1e-4

    def update(self, x):
        batch_mean = np.mean(x, axis=0)
        batch_var = np.var(x, axis=0)
        batch_count = x.shape[0]
        
        delta = batch_mean - self.mean
        tot_count = self.count + batch_count
        
        m_a = self.var * self.count
        m_b = batch_var * batch_count
        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / tot_count
        
        self.mean = self.mean + delta * batch_count / tot_count
        self.var = M2 / tot_count
        self.count = tot_count

    def normalize(self, x):
        return (x - self.mean) / (np.sqrt(self.var) + 1e-8)
```

This `RunningMeanStd` class maintains a running estimate of the
observation mean and variance and uses it to normalize inputs. Each
update takes a mini-batch $x$ and combines the batch statistics with the
existing running statistics using a numerically stable parallel update
rule. Concretely, if the current running stats are $(\mu, \sigma^2, n)$
and the new batch stats are $(\mu_b, \sigma_b^2, n_b)$, the combined
mean is $$\mu' = \mu + \frac{n_b}{n+n_b}(\mu_b-\mu).$$ To update the
variance stably, we track $M_2$, the total sum of squared deviations
from the mean: $$M_2 = \sum_{i=1}^{n} (x_i-\mu)^2,
\qquad
M_{2,b} = \sum_{i=1}^{n_b} (x_i-\mu_b)^2.$$ These let us merge variances
without storing old data:
$$M_2' = M_2 + M_{2,b} + \frac{(\mu_b-\mu)^2\,n\,n_b}{n+n_b},
\qquad
\sigma'^2 = \frac{M_2'}{n+n_b}.$$ Finally, normalization is done
elementwise as $$\hat{x} = \frac{x-\mu}{\sqrt{\sigma^2}+\epsilon}.$$
Instantiate an object of type RunningMeanStd, and use it to normalize
the observations, like so:

``` python
obs_normalizer = RunningMeanStd(shape=(obs_dim,))
...
obs, info = env.reset()
obs = obs_normalizer.normalize(obs)
...
next_obs, reward, terminated, truncated, info = env.step(env_action)
obs_normalizer.update(np.array([next_obs]))
next_obs = obs_normalizer.normalize(next_obs)
```

## Complete Code for Humanoid

The full implementation is available in Appendix B and on GitHub Gist:\
<https://gist.github.com/yeshwantd/93ad6f62cd78f9edb6788679d7138773>\
\
Here is a short demo of the trained policy on Humanoid:\
<https://youtu.be/FYF69FzfnK0>

# Full Implementation of SAC for LunarLanderContinuous

``` python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal

import copy
import numpy as np
import gymnasium as gym
from collections import deque
import random
import time
import os
import sys

class PolicyNetwork(nn.Module):
    """
    Parametrized policy πθ(a|s).
    Uses a Gaussian distribution for continuous actions.
    """
    def __init__(self):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(8, 256)
        self.fc2 = nn.Linear(256, 256)
        self.mean = nn.Linear(256, 2) # Mean of the Gaussian
        self.log_std = nn.Linear(256, 2) # Log-standard deviation of the Gaussian

    def forward(self, x):
        """
        Computes the mean and log-std of the Gaussian distribution.
        """
        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        mean = self.mean(x) 
        log_std = self.log_std(x)
        # Clamp log_std to [min_val, max_val] for numerical stability
        log_std = torch.clamp(log_std, min=-20, max=2) 
        return mean, log_std

    def sample(self, x):
        """
        Samples an action using the reparameterization trick and applies tanh squashing.
        Returns:
            action: Squashed action in [-1, 1]
            log_prob: Corrected log-probability of the action
        """
        mean, log_std = self.forward(x)
        std = log_std.exp()
        normal = Normal(mean, std)
        
        # Reparameterization trick: u = mean + std * epsilon, where epsilon ~ N(0, 1)
        u = normal.rsample()  
        action = torch.tanh(u) # Enforce action bounds from -1 to 1

        # Calculate log probability of the action
        log_prob = normal.log_prob(u)
        # Apply Jacobian correction for the tanh transformation
        # log π(a|s) = log µ(u|s) - sum(log(1 - tanh^2(u)))
        log_prob -= torch.log(1 - action.pow(2) + 1e-6) 
        log_prob = log_prob.sum(1, keepdim=True)
        return action, log_prob
        
class QNetwork(nn.Module):
    """
    Soft Q-function Qφ(s, a).
    Computes the expected return of taking action 'a' in state 's'.
    """
    def __init__(self):
        super(QNetwork, self).__init__()
        # Input: obs_dim (8) + action_dim (2) = 10
        self.fc1 = nn.Linear(10, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 1)
        
    def forward(self, x):
        """
        x is a concatenation of [observation, action]
        """
        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        x = self.fc3(x)
        return x
        
def train(checkpoint_path):
    # Training and test configs
    epochs = 2000
    test_epochs_freq = 100
    steps_per_epoch = 256
    batch_size = 256
    num_test_episodes = 10
    num_trajectories = 10
    best_test_reward = 0

    # Reproducibility
    train_seed = 42
    test_seed = 55
    set_seed = False if "--no-seed" in sys.argv else True
    if set_seed and train_seed is not None:
        torch.manual_seed(train_seed)
        np.random.seed(train_seed)
        random.seed(train_seed)

    # Environment configs
    env_name = "LunarLanderContinuous-v3"
    max_episode_steps = 500
    
    # Agent configs
    gamma = 0.99
    entropy_weight = 0.05
    policy_lr = 3e-4
    q_lr = 3e-4
    
    # Replay buffer configs
    replay_buffer_size = 100000
    
    # Target network configs
    tau = 0.005

    # Noise configs
    noise_mean = 0
    noise_std = 0.2
    noise_std_min = 0.05
    noise_decay_steps = 50000
    
    # Initialize the environment
    env = gym.make(env_name, max_episode_steps=max_episode_steps)
    
    # Initialize the actor and critic networks
    policy = PolicyNetwork()
    q1 = QNetwork()
    q2 = QNetwork()
    
    # Initialize the target networks and set parameters equal to the original networks
    # policy_target = copy.deepcopy(policy)
    q1_target = copy.deepcopy(q1)
    q2_target = copy.deepcopy(q2)
    
    # Initialize the optimizers
    policy_optim = optim.Adam(policy.parameters(), lr=policy_lr)
    q1_optim = optim.Adam(q1.parameters(), lr=q_lr)
    q2_optim = optim.Adam(q2.parameters(), lr=q_lr)
    
    # Initialize the replay buffer
    replay_buffer = deque(maxlen=replay_buffer_size)
    
    # Training loop
    for epoch in range(epochs):
        # Collect a batch of trajectories
        for i in range(num_trajectories):
            seed = train_seed + epoch if set_seed and train_seed is not None else None
            obs, info = env.reset(seed=seed)
            done = False
            while not done:
                # Sample an action from the policy
                with torch.no_grad():
                    action, _ = policy.sample(torch.tensor(obs, dtype=torch.float32).unsqueeze(0))
                action = action.squeeze(0).numpy()
                next_obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                replay_buffer.append((obs, action, reward, next_obs, done))
                obs = next_obs
        
        # Sample a batch of steps from the replay buffer
        for i in range(steps_per_epoch):
            if len(replay_buffer) < batch_size:
                break
            batch = random.sample(replay_buffer, batch_size)
            obs, action, reward, next_obs, done = zip(*batch)
            
            # Convert to tensors
            obs = torch.tensor(np.array(obs), dtype=torch.float32)
            action = torch.tensor(np.array(action), dtype=torch.float32)
            reward = torch.tensor(np.array(reward), dtype=torch.float32).unsqueeze(1)
            next_obs = torch.tensor(np.array(next_obs), dtype=torch.float32)
            done = torch.tensor(np.array(done), dtype=torch.int8).unsqueeze(1)
            
            # Compute targets for Q-functions
            with torch.no_grad():
                # Sample next actions and their log-probabilities from current policy
                next_action, next_log_prob = policy.sample(next_obs)
                next_state_action = torch.cat([next_obs, next_action], dim=1)
                
                # Clipped Double-Q trick: use the minimum of two target Q-networks
                # This reduces overestimation bias in Q-learning.
                q1_next = q1_target(next_state_action)
                q2_next = q2_target(next_state_action)
                min_q_next = torch.min(q1_next, q2_next)
                
                # Bellman equation with entropy term:
                # y = r + γ * (1 - d) * (Q_target(s', a') - α * log_π(a'|s'))
                y = reward + gamma * (1 - done) * (min_q_next - entropy_weight * next_log_prob)

            # Update Q-functions
            state_action = torch.cat([obs, action], dim=1)
            q1_loss = F.mse_loss(q1(state_action), y)  # J(φ) = E(s,a) ~ D [1/2 * (Qφ(s,a) - y)^2]
            q2_loss = F.mse_loss(q2(state_action), y)
            
            q1_optim.zero_grad()
            q1_loss.backward()
            q1_optim.step()

            q2_optim.zero_grad()
            q2_loss.backward()
            q2_optim.step()

            # Update Policy
            # Sample current actions using current policy
            new_action, log_prob = policy.sample(obs)
            q1_new = q1(torch.cat([obs, new_action], dim=1))
            q2_new = q2(torch.cat([obs, new_action], dim=1))
            # Use the minimum of current Q-networks for the policy objective
            min_q_new = torch.min(q1_new, q2_new)
            
            # Policy objective: Maximize (Q - α * log_π)
            # We minimize -J(θ) = -E [Qφ(s, a) - α * log(πθ(a|s))]
            policy_loss = -(min_q_new - entropy_weight * log_prob).mean()  

            policy_optim.zero_grad()
            policy_loss.backward()
            policy_optim.step()

            # Soft update target networks using Exponential Moving Average (EMA)
            # Q_target = τ * Q_online + (1 - τ) * Q_target
            for target_param, param in zip(q1_target.parameters(), q1.parameters()):
                target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
            
            for target_param, param in zip(q2_target.parameters(), q2.parameters()):
                target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)

        # Test the policy
        if (epoch + 1) % test_epochs_freq == 0:
            test_rewards = []
            for i in range(num_test_episodes):
                obs, info = env.reset(seed = test_seed + i if set_seed and test_seed is not None else None)
                done = False
                episode_reward = 0
                while not done:
                    with torch.no_grad():
                        action, _ = policy.sample(torch.tensor(obs, dtype=torch.float32).unsqueeze(0))
                    action = action.squeeze(0).numpy()
                    obs, reward, terminated, truncated, info = env.step(action)
                    done = terminated or truncated
                    episode_reward += reward
                if episode_reward > best_test_reward:
                    best_test_reward = episode_reward
                    torch.save(policy.state_dict(), checkpoint_path)
                        

                test_rewards.append(episode_reward)
            print(f"Epoch {epoch+1}, Avg Test Reward: {np.mean(test_rewards)}")

    env.close()

# Demo the policy
def demo(policy, num_times):
    env = gym.make("LunarLanderContinuous-v3", render_mode="human")
    for i in range(num_times):    
        obs, info = env.reset()
        done = False
        tot_reward = 0
        policy.eval()
        with torch.no_grad():
            while not done:
                action, _ = policy.sample(torch.tensor(obs, dtype=torch.float32).unsqueeze(0))
                action = action.squeeze(0).numpy()
                obs, reward, terminated, truncated, info = env.step(action)
                tot_reward += reward
                done = terminated or truncated
            print(f"Total reward for episode {i+1}: {tot_reward}")

if __name__ == "__main__":
    train_flag = False
    render_flag = True
    checkpoint_path = "checkpoints/sac_policy.pth"
    if train_flag:
        start_time = time.time()
        train(checkpoint_path)
        end_time = time.time()
        print(f"Total Training Time: {(end_time - start_time)/60:.2f} minutes")
    elif render_flag:
        policy = PolicyNetwork()
        policy.load_state_dict(torch.load(checkpoint_path))
        demo(policy, 5)
```

# Full Implementation of SAC for Humaoid

``` python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal

import copy
import numpy as np
import gymnasium as gym
from collections import deque
import random
import time
import os
import sys
import pickle

class RunningMeanStd:
    """
    Tracks the running mean and variance of a data stream.
    Used for observation normalization.
    """
    def __init__(self, shape):
        self.mean = np.zeros(shape)
        self.var = np.ones(shape)
        self.count = 1e-4

    def update(self, x):
        """
        Updates the running mean and variance with a new batch of data.
        """
        batch_mean = np.mean(x, axis=0)
        batch_var = np.var(x, axis=0)
        batch_count = x.shape[0]
        
        delta = batch_mean - self.mean
        tot_count = self.count + batch_count
        
        m_a = self.var * self.count
        m_b = batch_var * batch_count
        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / tot_count
        
        self.mean = self.mean + delta * batch_count / tot_count
        self.var = M2 / tot_count
        self.count = tot_count

    def normalize(self, x):
        """
        Normalizes the input using the running mean and standard deviation.
        """
        return (x - self.mean) / (np.sqrt(self.var) + 1e-8)

class PolicyNetwork(nn.Module):
    """
    Parametrized policy πθ(a|s).
    Uses a Gaussian distribution for continuous actions.
    """
    def __init__(self, obs_dim, action_dim):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(obs_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.mean = nn.Linear(256, action_dim) 
        self.log_std = nn.Linear(256, action_dim)

    def forward(self, x):
        """
        Computes the mean and log-std of the Gaussian distribution.
        """
        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        mean = self.mean(x) 
        log_std = self.log_std(x)
        # Clamp log_std to [min_val, max_val] for numerical stability
        log_std = torch.clamp(log_std, min=-20, max=2) 
        return mean, log_std

    def sample(self, x):
        """
        Samples an action using the reparameterization trick and applies tanh squashing.
        Returns:
            action: Squashed action in [-1, 1]
            log_prob: Corrected log-probability of the action
        """
        mean, log_std = self.forward(x)
        std = log_std.exp()
        normal = Normal(mean, std)
        
        # Reparameterization trick: u = mean + std * epsilon, where epsilon ~ N(0, 1)
        u = normal.rsample()  
        action = torch.tanh(u) # Enforce action bounds from -1 to 1

        # Calculate log probability of the action
        log_prob = normal.log_prob(u)
        # Apply Jacobian correction for the tanh transformation
        # log π(a|s) = log µ(u|s) - sum(log(1 - tanh^2(u)))
        log_prob -= torch.log(1 - action.pow(2) + 1e-6) 
        log_prob = log_prob.sum(1, keepdim=True)
        return action, log_prob
        
class QNetwork(nn.Module):
    """
    Soft Q-function Qφ(s, a).
    Computes the expected return of taking action 'a' in state 's'.
    """
    def __init__(self, obs_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(obs_dim + action_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 1)
        
    def forward(self, x):
        """
        x is a concatenation of [observation, action]
        """
        x = F.leaky_relu(self.fc1(x))
        x = F.leaky_relu(self.fc2(x))
        x = self.fc3(x)
        return x
        
def train(checkpoint_path):
    # Training and test configs
    epochs = 2000
    test_epochs_freq = 100
    steps_per_epoch = 256
    batch_size = 256
    num_test_episodes = 10
    num_trajectories = 10
    best_test_reward = 5000

    # Reproducibility
    train_seed = 42
    test_seed = 55
    set_seed = False if "--no-seed" in sys.argv else True
    if set_seed and train_seed is not None:
        torch.manual_seed(train_seed)
        np.random.seed(train_seed)
        random.seed(train_seed)

    # Environment configs
    env_name = "Humanoid-v5"
    max_episode_steps = 1000
    
    # Agent configs
    gamma = 0.99
    policy_lr = 3e-4
    q_lr = 3e-4
    alpha_lr = 3e-4
    reward_scale = 1.0
    
    # Replay buffer configs
    replay_buffer_size = 100000
    
    # Target network configs
    tau = 0.005


    # Initialize the environment
    env = gym.make(env_name, max_episode_steps=max_episode_steps)
    obs_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    
    # Action space bounds for scaling
    action_low = torch.tensor(env.action_space.low, dtype=torch.float32)
    action_high = torch.tensor(env.action_space.high, dtype=torch.float32)
    
    obs_normalizer = RunningMeanStd(shape=(obs_dim,))
    
    # Initialize the actor and critic networks
    policy = PolicyNetwork(obs_dim, action_dim)
    q1 = QNetwork(obs_dim, action_dim)
    q2 = QNetwork(obs_dim, action_dim)
    
    # Initialize the target networks and set parameters equal to the original networks
    q1_target = copy.deepcopy(q1)
    q2_target = copy.deepcopy(q2)
    
    # Initialize the optimizers
    policy_optim = optim.Adam(policy.parameters(), lr=policy_lr)
    q1_optim = optim.Adam(q1.parameters(), lr=q_lr)
    q2_optim = optim.Adam(q2.parameters(), lr=q_lr)

    # Automatic entropy tuning
    target_entropy = -torch.prod(torch.Tensor(env.action_space.shape).to(torch.device("cpu"))).item()
    log_alpha = torch.zeros(1, requires_grad=True)
    alpha_optim = optim.Adam([log_alpha], lr=alpha_lr)
    alpha = log_alpha.exp()
    
    # Initialize the replay buffer
    replay_buffer = deque(maxlen=replay_buffer_size)
    
    # Training loop
    for epoch in range(epochs):
        # Collect a batch of trajectories
        for i in range(num_trajectories):
            seed = train_seed + epoch if set_seed and train_seed is not None else None
            obs, info = env.reset(seed=seed)
            obs = obs_normalizer.normalize(obs)
            done = False
            while not done:
                # Sample an action from the policy (in [-1, 1] range)
                with torch.no_grad():
                    action, _ = policy.sample(torch.tensor(obs, dtype=torch.float32).unsqueeze(0))
                action = action.squeeze(0).cpu().numpy()
                
                # Scale action to environment bounds: [-1, 1] -> [low, high]
                env_action = env.action_space.low + (action + 1.0) * 0.5 * (env.action_space.high - env.action_space.low)
                
                next_obs, reward, terminated, truncated, info = env.step(env_action)
                
                # Update normalizer with raw next_obs, then normalize it
                obs_normalizer.update(np.array([next_obs]))
                next_obs = obs_normalizer.normalize(next_obs)
                
                done = terminated or truncated
                # Store unscaled action in replay buffer for consistent network logic
                replay_buffer.append((obs, action, reward * reward_scale, next_obs, done))
                obs = next_obs
        
        # Sample a batch of steps from the replay buffer
        for i in range(steps_per_epoch):
            if len(replay_buffer) < batch_size:
                break
            batch = random.sample(replay_buffer, batch_size)
            b_obs, b_action, b_reward, b_next_obs, b_done = zip(*batch)
            
            # Convert to tensors
            b_obs = torch.tensor(np.array(b_obs), dtype=torch.float32)
            b_action = torch.tensor(np.array(b_action), dtype=torch.float32)
            b_reward = torch.tensor(np.array(b_reward), dtype=torch.float32).unsqueeze(1)
            b_next_obs = torch.tensor(np.array(b_next_obs), dtype=torch.float32)
            b_done = torch.tensor(np.array(b_done), dtype=torch.int8).unsqueeze(1)
            
            # Compute targets for Q-functions
            with torch.no_grad():
                # Sample next actions and their log-probabilities from current policy
                next_action, next_log_prob = policy.sample(b_next_obs)
                next_state_action = torch.cat([b_next_obs, next_action], dim=1)
                
                # Clipped Double-Q trick: use the minimum of two target Q-networks
                # This reduces overestimation bias in Q-learning.
                q1_next = q1_target(next_state_action)
                q2_next = q2_target(next_state_action)
                min_q_next = torch.min(q1_next, q2_next)
                
                # Bellman equation with entropy term:
                # y = r + γ * (1 - d) * (Q_target(s', a') - α * log_π(a'|s'))
                y = b_reward + gamma * (1 - b_done) * (min_q_next - alpha * next_log_prob)

            # Update Q-functions
            state_action = torch.cat([b_obs, b_action], dim=1)
            q1_loss = F.mse_loss(q1(state_action), y)  # J(φ) = E [1/2 * (Qφ(s,a) - y)^2]
            q2_loss = F.mse_loss(q2(state_action), y)
            
            q1_optim.zero_grad()
            q1_loss.backward()
            q1_optim.step()

            q2_optim.zero_grad()
            q2_loss.backward()
            q2_optim.step()

            # Update Policy
            # Sample current actions using current policy
            new_action, log_prob = policy.sample(b_obs)
            q1_new = q1(torch.cat([b_obs, new_action], dim=1))
            q2_new = q2(torch.cat([b_obs, new_action], dim=1))
            # Use the minimum of current Q-networks for the policy objective
            min_q_new = torch.min(q1_new, q2_new)
            
            # Policy objective: Maximize (Q - α * log_π)
            # We minimize -J(θ) = -E [Qφ(s, a) - α * log(πθ(a|s))]
            policy_loss = -(min_q_new - alpha * log_prob).mean()
            
            policy_optim.zero_grad()
            policy_loss.backward()
            policy_optim.step()

            # Update Alpha (entropy weight)
            alpha_loss = -(log_alpha * (log_prob + target_entropy).detach()).mean()
            alpha_optim.zero_grad()
            alpha_loss.backward()
            alpha_optim.step()
            alpha = log_alpha.exp()

            # Soft update target networks using Exponential Moving Average (EMA)
            # Q_target = τ * Q_online + (1 - τ) * Q_target
            for target_param, param in zip(q1_target.parameters(), q1.parameters()):
                target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
            for target_param, param in zip(q2_target.parameters(), q2.parameters()):
                target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)
        
        # Test the policy
        if (epoch + 1) % test_epochs_freq == 0:
            test_rewards = []
            for i in range(num_test_episodes):
                obs, info = env.reset(seed = test_seed + i if set_seed and test_seed is not None else None)
                obs = obs_normalizer.normalize(obs)
                done = False
                episode_reward = 0
                while not done:
                    with torch.no_grad():
                        action, _ = policy.sample(torch.tensor(obs, dtype=torch.float32).unsqueeze(0))
                    action = action.squeeze(0).cpu().numpy()
                    
                    # Scale action to environment bounds: [-1, 1] -> [low, high]
                    env_action = env.action_space.low + (action + 1.0) * 0.5 * (env.action_space.high - env.action_space.low)
                    
                    obs, reward, terminated, truncated, info = env.step(env_action)
                    obs = obs_normalizer.normalize(obs)
                    done = terminated or truncated
                    episode_reward += reward
                if episode_reward > best_test_reward:
                    best_test_reward = episode_reward
                    torch.save(policy.state_dict(), checkpoint_path)
                    with open("checkpoints/sac_humanoid_obs_normalizer.pkl", "wb") as f:
                        pickle.dump(obs_normalizer, f)
                test_rewards.append(episode_reward)
            print(f"Epoch {epoch+1}, Avg Test Reward: {np.mean(test_rewards)}")

    env.close()

# Demo the policy
def demo(policy, obs_normalizer, num_times):
    env = gym.make("Humanoid-v5", render_mode="human")
    for i in range(num_times):    
        obs, info = env.reset()
        if obs_normalizer is not None:
            obs = obs_normalizer.normalize(obs)
        done = False
        tot_reward = 0
        policy.eval()
        with torch.no_grad():
            while not done:
                action, _ = policy.sample(torch.tensor(obs, dtype=torch.float32).unsqueeze(0))
                action = action.squeeze(0).cpu().numpy()
                
                # Scale action to environment bounds: [-1, 1] -> [low, high]
                env_action = env.action_space.low + (action + 1.0) * 0.5 * (env.action_space.high - env.action_space.low)
                
                obs, reward, terminated, truncated, info = env.step(env_action)
                if obs_normalizer is not None:
                    obs = obs_normalizer.normalize(obs)
                tot_reward += reward
                done = terminated or truncated
            print(f"Total reward for episode {i+1}: {tot_reward}")
    env.close()

if __name__ == "__main__":
    train_flag = True  
    render_flag = False
    checkpoint_path = "checkpoints/sac_humanoid_policy.pth"
    if train_flag:
        start_time = time.time()
        train(checkpoint_path)
        end_time = time.time()
        print(f"Total Training Time: {(end_time - start_time)/60:.2f} minutes")
    elif render_flag:
        env = gym.make("Humanoid-v5", render_mode="human")
        obs_dim = env.observation_space.shape[0]
        action_dim = env.action_space.shape[0]
        env.close()
        
        policy = PolicyNetwork(obs_dim, action_dim)
        policy.load_state_dict(torch.load(checkpoint_path))
        
        obs_normalizer = None
        if os.path.exists("checkpoints/sac_humanoid_obs_normalizer.pkl"):
            with open("checkpoints/sac_humanoid_obs_normalizer.pkl", "rb") as f:
                obs_normalizer = pickle.load(f)
                
        demo(policy, obs_normalizer, 5)
```
